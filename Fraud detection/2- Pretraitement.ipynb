{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Distributing\n",
    "- Scaling = All Vs in our data are scaled, but Time and Amount Classes are not\n",
    "- Ditributiong = Fixing Class Imbalance by doing subsampling of equal classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Clarrification for data transformation methods : \n",
    "- Normalization = Min max scaling, when no outliers, cannot cope with them\n",
    "- Standardization = Z score normalization, when data follows gaussian distribution\n",
    "- Scaling methods = Normalization AND Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling avoids overfitting AND getting wrong correlations\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "r_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = r_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = r_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "y = df['Class']\n",
    "df.drop(['Amount','Time'],axis=1,inplace=True)\n",
    "df.drop(['Class'],axis=1,inplace=True)\n",
    "df['Class'] = y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  0.090794  ...  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024 -0.054952  ...  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  0.753074  ...  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  scaled_amount  scaled_time  Class  \n",
       "0 -0.189115  0.133558 -0.021053       1.783274    -0.994983      0  \n",
       "1  0.125895 -0.008983  0.014724      -0.269825    -0.994983      0  \n",
       "2 -0.139097 -0.055353 -0.059752       4.983721    -0.994972      0  \n",
       "3 -0.221929  0.062723  0.061458       1.418291    -0.994972      0  \n",
       "4  0.502292  0.219422  0.215153       0.670579    -0.994960      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we try to split data normally we get an issue but we try quand meme parce qu'on est tetue et bornee\n",
    "\n",
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#using a k fold variation to make 5 splits of train and test with a balanced amount of both classes we have\n",
    "s = StratifiedKFold(n_splits=5,shuffle=False,random_state=None)\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227845 56962\n",
      "227845 56962\n",
      "227846 56961\n",
      "227846 56961\n",
      "227846 56961\n"
     ]
    }
   ],
   "source": [
    "for train_ind, test_ind in s.split(X,y):\n",
    "    #print(\"Train:\", train_ind, \"Test:\", test_ind)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_ind], X.iloc[test_ind]\n",
    "    original_ytrain, original_ytest = y.iloc[train_ind], y.iloc[test_ind]\n",
    "    # a chaque fois we shuffle our data to get a new fold\n",
    "    print(len(original_Xtrain), len(original_Xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distributions: \n",
      "\n",
      "[0.99827076 0.00172924]\n",
      "[0.99827952 0.00172048]\n"
     ]
    }
   ],
   "source": [
    "## showing the high imbalance\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution s: Random under-sampling, Random over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not so Random Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see ration \n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64272</th>\n",
       "      <td>1.453187</td>\n",
       "      <td>-1.495856</td>\n",
       "      <td>-0.977913</td>\n",
       "      <td>-2.270149</td>\n",
       "      <td>0.792376</td>\n",
       "      <td>3.572262</td>\n",
       "      <td>-1.569487</td>\n",
       "      <td>0.841158</td>\n",
       "      <td>-1.570196</td>\n",
       "      <td>1.320589</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.079484</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.974958</td>\n",
       "      <td>0.471812</td>\n",
       "      <td>-0.321274</td>\n",
       "      <td>0.039857</td>\n",
       "      <td>0.024301</td>\n",
       "      <td>0.537972</td>\n",
       "      <td>-0.394730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123301</th>\n",
       "      <td>-1.298359</td>\n",
       "      <td>1.079671</td>\n",
       "      <td>-0.180678</td>\n",
       "      <td>1.287839</td>\n",
       "      <td>1.858273</td>\n",
       "      <td>-2.223695</td>\n",
       "      <td>0.525167</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>-0.168893</td>\n",
       "      <td>-2.544410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.851270</td>\n",
       "      <td>-0.370800</td>\n",
       "      <td>0.298242</td>\n",
       "      <td>0.442930</td>\n",
       "      <td>-0.522832</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.135698</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.091824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281053</th>\n",
       "      <td>-1.219123</td>\n",
       "      <td>1.489952</td>\n",
       "      <td>-0.187655</td>\n",
       "      <td>-0.475759</td>\n",
       "      <td>0.959856</td>\n",
       "      <td>-1.389524</td>\n",
       "      <td>0.876273</td>\n",
       "      <td>0.186469</td>\n",
       "      <td>-0.448640</td>\n",
       "      <td>-2.736630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090943</td>\n",
       "      <td>-0.529271</td>\n",
       "      <td>-0.318949</td>\n",
       "      <td>1.015782</td>\n",
       "      <td>-0.381530</td>\n",
       "      <td>-0.037237</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.062461</td>\n",
       "      <td>1.001151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261925</th>\n",
       "      <td>-2.783865</td>\n",
       "      <td>1.596824</td>\n",
       "      <td>-2.084844</td>\n",
       "      <td>2.512986</td>\n",
       "      <td>-1.446749</td>\n",
       "      <td>-0.828496</td>\n",
       "      <td>-0.732262</td>\n",
       "      <td>-0.203329</td>\n",
       "      <td>-0.347046</td>\n",
       "      <td>-2.162061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293268</td>\n",
       "      <td>0.199568</td>\n",
       "      <td>0.146868</td>\n",
       "      <td>0.163602</td>\n",
       "      <td>-0.624085</td>\n",
       "      <td>-1.333100</td>\n",
       "      <td>0.428634</td>\n",
       "      <td>1.872424</td>\n",
       "      <td>0.887593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123238</th>\n",
       "      <td>1.140208</td>\n",
       "      <td>1.156431</td>\n",
       "      <td>-1.471578</td>\n",
       "      <td>2.076278</td>\n",
       "      <td>0.774809</td>\n",
       "      <td>-1.002532</td>\n",
       "      <td>0.264948</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.248835</td>\n",
       "      <td>-2.100667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866812</td>\n",
       "      <td>-0.121583</td>\n",
       "      <td>-0.356109</td>\n",
       "      <td>0.634573</td>\n",
       "      <td>-0.306311</td>\n",
       "      <td>0.094087</td>\n",
       "      <td>0.121065</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.092048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "64272   1.453187 -1.495856 -0.977913 -2.270149  0.792376  3.572262 -1.569487   \n",
       "123301 -1.298359  1.079671 -0.180678  1.287839  1.858273 -2.223695  0.525167   \n",
       "281053 -1.219123  1.489952 -0.187655 -0.475759  0.959856 -1.389524  0.876273   \n",
       "261925 -2.783865  1.596824 -2.084844  2.512986 -1.446749 -0.828496 -0.732262   \n",
       "123238  1.140208  1.156431 -1.471578  2.076278  0.774809 -1.002532  0.264948   \n",
       "\n",
       "              V8        V9       V10  ...       V22       V23       V24  \\\n",
       "64272   0.841158 -1.570196  1.320589  ... -1.079484  0.032946  0.974958   \n",
       "123301 -0.096874 -0.168893 -2.544410  ... -0.851270 -0.370800  0.298242   \n",
       "281053  0.186469 -0.448640 -2.736630  ... -0.090943 -0.529271 -0.318949   \n",
       "261925 -0.203329 -0.347046 -2.162061  ...  0.293268  0.199568  0.146868   \n",
       "123238  0.013162  0.248835 -2.100667  ... -0.866812 -0.121583 -0.356109   \n",
       "\n",
       "             V25       V26       V27       V28  scaled_amount  scaled_time  \\\n",
       "64272   0.471812 -0.321274  0.039857  0.024301       0.537972    -0.394730   \n",
       "123301  0.442930 -0.522832  0.000105  0.135698      -0.293440    -0.091824   \n",
       "281053  1.015782 -0.381530 -0.037237  0.013648       0.062461     1.001151   \n",
       "261925  0.163602 -0.624085 -1.333100  0.428634       1.872424     0.887593   \n",
       "123238  0.634573 -0.306311  0.094087  0.121065      -0.293440    -0.092048   \n",
       "\n",
       "        Class  \n",
       "64272       0  \n",
       "123301      1  \n",
       "281053      0  \n",
       "261925      1  \n",
       "123238      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we will use 492 instances for both classes \n",
    "df = df.sample(frac=1) #shuffling all our data\n",
    "\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492] # on prend only same amount of fraud samples\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df]) # this is our balanced dataset\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42) # we shuffle our balanced dataset\n",
    "new_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    0.5\n",
       "1    0.5\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing distribution now \n",
    "new_df['Class'].value_counts()/len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Geting outliers from the other class (fraud) \n",
    "\n",
    "def outliers(data, col, multiplier = 1.5):\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q2 = data[col].quantile(0.50)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    Q4 = data[col].quantile(1)\n",
    "    \n",
    "    IQR = Q3 = Q1\n",
    "    \n",
    "    bas = Q1 - multiplier * IQR\n",
    "    haut = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = data[(data[col] < bas) | (data[col] > haut)]\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outliers(new_df,'V4')) / len(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64272</th>\n",
       "      <td>1.453187</td>\n",
       "      <td>-1.495856</td>\n",
       "      <td>-0.977913</td>\n",
       "      <td>-2.270149</td>\n",
       "      <td>0.792376</td>\n",
       "      <td>3.572262</td>\n",
       "      <td>-1.569487</td>\n",
       "      <td>0.841158</td>\n",
       "      <td>-1.570196</td>\n",
       "      <td>1.320589</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.079484</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.974958</td>\n",
       "      <td>0.471812</td>\n",
       "      <td>-0.321274</td>\n",
       "      <td>0.039857</td>\n",
       "      <td>0.024301</td>\n",
       "      <td>0.537972</td>\n",
       "      <td>-0.394730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123301</th>\n",
       "      <td>-1.298359</td>\n",
       "      <td>1.079671</td>\n",
       "      <td>-0.180678</td>\n",
       "      <td>1.287839</td>\n",
       "      <td>1.858273</td>\n",
       "      <td>-2.223695</td>\n",
       "      <td>0.525167</td>\n",
       "      <td>-0.096874</td>\n",
       "      <td>-0.168893</td>\n",
       "      <td>-2.544410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.851270</td>\n",
       "      <td>-0.370800</td>\n",
       "      <td>0.298242</td>\n",
       "      <td>0.442930</td>\n",
       "      <td>-0.522832</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.135698</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.091824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281053</th>\n",
       "      <td>-1.219123</td>\n",
       "      <td>1.489952</td>\n",
       "      <td>-0.187655</td>\n",
       "      <td>-0.475759</td>\n",
       "      <td>0.959856</td>\n",
       "      <td>-1.389524</td>\n",
       "      <td>0.876273</td>\n",
       "      <td>0.186469</td>\n",
       "      <td>-0.448640</td>\n",
       "      <td>-2.736630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090943</td>\n",
       "      <td>-0.529271</td>\n",
       "      <td>-0.318949</td>\n",
       "      <td>1.015782</td>\n",
       "      <td>-0.381530</td>\n",
       "      <td>-0.037237</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.062461</td>\n",
       "      <td>1.001151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261925</th>\n",
       "      <td>-2.783865</td>\n",
       "      <td>1.596824</td>\n",
       "      <td>-2.084844</td>\n",
       "      <td>2.512986</td>\n",
       "      <td>-1.446749</td>\n",
       "      <td>-0.828496</td>\n",
       "      <td>-0.732262</td>\n",
       "      <td>-0.203329</td>\n",
       "      <td>-0.347046</td>\n",
       "      <td>-2.162061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293268</td>\n",
       "      <td>0.199568</td>\n",
       "      <td>0.146868</td>\n",
       "      <td>0.163602</td>\n",
       "      <td>-0.624085</td>\n",
       "      <td>-1.333100</td>\n",
       "      <td>0.428634</td>\n",
       "      <td>1.872424</td>\n",
       "      <td>0.887593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123238</th>\n",
       "      <td>1.140208</td>\n",
       "      <td>1.156431</td>\n",
       "      <td>-1.471578</td>\n",
       "      <td>2.076278</td>\n",
       "      <td>0.774809</td>\n",
       "      <td>-1.002532</td>\n",
       "      <td>0.264948</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.248835</td>\n",
       "      <td>-2.100667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866812</td>\n",
       "      <td>-0.121583</td>\n",
       "      <td>-0.356109</td>\n",
       "      <td>0.634573</td>\n",
       "      <td>-0.306311</td>\n",
       "      <td>0.094087</td>\n",
       "      <td>0.121065</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.092048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "64272   1.453187 -1.495856 -0.977913 -2.270149  0.792376  3.572262 -1.569487   \n",
       "123301 -1.298359  1.079671 -0.180678  1.287839  1.858273 -2.223695  0.525167   \n",
       "281053 -1.219123  1.489952 -0.187655 -0.475759  0.959856 -1.389524  0.876273   \n",
       "261925 -2.783865  1.596824 -2.084844  2.512986 -1.446749 -0.828496 -0.732262   \n",
       "123238  1.140208  1.156431 -1.471578  2.076278  0.774809 -1.002532  0.264948   \n",
       "\n",
       "              V8        V9       V10  ...       V22       V23       V24  \\\n",
       "64272   0.841158 -1.570196  1.320589  ... -1.079484  0.032946  0.974958   \n",
       "123301 -0.096874 -0.168893 -2.544410  ... -0.851270 -0.370800  0.298242   \n",
       "281053  0.186469 -0.448640 -2.736630  ... -0.090943 -0.529271 -0.318949   \n",
       "261925 -0.203329 -0.347046 -2.162061  ...  0.293268  0.199568  0.146868   \n",
       "123238  0.013162  0.248835 -2.100667  ... -0.866812 -0.121583 -0.356109   \n",
       "\n",
       "             V25       V26       V27       V28  scaled_amount  scaled_time  \\\n",
       "64272   0.471812 -0.321274  0.039857  0.024301       0.537972    -0.394730   \n",
       "123301  0.442930 -0.522832  0.000105  0.135698      -0.293440    -0.091824   \n",
       "281053  1.015782 -0.381530 -0.037237  0.013648       0.062461     1.001151   \n",
       "261925  0.163602 -0.624085 -1.333100  0.428634       1.872424     0.887593   \n",
       "123238  0.634573 -0.306311  0.094087  0.121065      -0.293440    -0.092048   \n",
       "\n",
       "        Class  \n",
       "64272       0  \n",
       "123301      1  \n",
       "281053      0  \n",
       "261925      1  \n",
       "123238      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am going to create a balanced dataset in which : the 492 instances of no fraud have no extreme outliers, this will help our model train well on no fraud data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df, target_size=492, outlier_removal_func=None, random_state=None):\n",
    "    df = df.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    fraud_df = df[df['Class'] == 1]\n",
    "    non_fraud_df = df[df['Class'] == 0]\n",
    "\n",
    "    balanced_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    fraud_df = fraud_df[:target_size]\n",
    "    balanced_df = pd.concat([balanced_df, fraud_df])\n",
    "\n",
    "    min_outliers = float('inf')\n",
    "    best_batch = None\n",
    "    for i in range(0, len(non_fraud_df), target_size):\n",
    "        non_fraud_df = non_fraud_df.sample(frac=1, random_state=random_state)\n",
    "        non_fraud_batch = non_fraud_df[i:i+target_size]\n",
    "\n",
    "        num_outliers = 0\n",
    "        if outlier_removal_func:\n",
    "            for col in df.columns[:-1]:  # Exclude 'Class' column\n",
    "                non_fraud_outliers_removed = outlier_removal_func(df, col)\n",
    "                num_outliers += len(non_fraud_outliers_removed.index)\n",
    "\n",
    "        if num_outliers < min_outliers:\n",
    "            min_outliers = num_outliers\n",
    "            best_batch = non_fraud_batch\n",
    "\n",
    "    balanced_df = pd.concat([balanced_df, best_batch])\n",
    "\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rania\\AppData\\Local\\Temp\\ipykernel_27364\\916350335.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_df = pd.concat([balanced_df, fraud_df])\n"
     ]
    }
   ],
   "source": [
    "balanced_dataset = create_balanced_dataset(new_df, target_size=492, outlier_removal_func=outliers, random_state=42)\n",
    "balanced_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outliers(balanced_dataset,'V4')) / len(balanced_dataset)\n",
    "# we have snot ignificantly reduced outliers proportion in dataset lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rania\\AppData\\Local\\Temp\\ipykernel_27364\\692864827.py:1: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x = 'Class', data=new_df, palette='Blues')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'Class', data=new_df, palette='Blues')\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('under_sampled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('creditcard_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df) # 492*2 for 2 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not so Random Over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nET DU COUP THIS WILL BE INCLUDED WHEN TRAINING OUR MODELS IN NOTEBOOK PREDICITON\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PS : this method creates synthetic data so we should do it during cross valisation to avoid data leakage\n",
    "'''\n",
    "ET DU COUP THIS WILL BE INCLUDED WHEN TRAINING OUR MODELS IN NOTEBOOK PREDICITON\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
